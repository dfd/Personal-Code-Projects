{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (688 only) [35pts]\n",
    "## Introduction\n",
    "\n",
    "In this problem you will develop two techniques for analyzing free text documents: a bag of words approach based upon creating a TFIDF matrix, and an n-gram language model.\n",
    "\n",
    "You'll be applying your models to the text from the Federalist Papers.  The Federalist papers were a series of essay written in 1787 and 1788 by Alexander Hamilton, James Madison, and John Jay (they were published anonymously at the time), that promoted the ratification of the U.S. Constitution.  If you're curious, you can read more about them here: https://en.wikipedia.org/wiki/The_Federalist_Papers . They are a particularly interesting data set, because although the authorship of most of the essays has been long known since around the deaths of Hamilton and Madison, there was still some question about the authorship of certain articles into the 20th century.  You'll use document vectors and language models to do this analysis for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "You'll use a copy of the Federalist Papers downloaded from Project Guttenberg, available here: http://www.gutenberg.org/ebooks/18 .  Specifically, the \"pg18.txt\" file included with the homework is the raw text downloaded from Project Guttenberg.  To ensure that everyone starts with the exact same corpus, we are providing you the code to load and tokenize this document, as given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_federalist_corpus(filename):\n",
    "    \"\"\" Load the federalist papers as a tokenized list of strings, one for each eassay\"\"\"\n",
    "    with open(filename, \"rt\") as f:\n",
    "        data = f.read()\n",
    "    papers = data.split(\"FEDERALIST\")\n",
    "    \n",
    "    # all start with \"To the people of the State of New York:\" (sometimes . instead of :)\n",
    "    # all end with PUBLIUS (or no end at all)\n",
    "    locations = [(i,[-1] + [m.end()+1 for m in re.finditer(r\"of the State of New York\", p)],\n",
    "                 [-1] + [m.start() for m in re.finditer(r\"PUBLIUS\", p)]) for i,p in enumerate(papers)]\n",
    "    papers_content = [papers[i][max(loc[1]):max(loc[2])] for i,loc in enumerate(locations)]\n",
    "\n",
    "    # discard entries that are not actually a paper\n",
    "    papers_content = [p for p in papers_content if len(p) > 0]\n",
    "\n",
    "    # replace all whitespace with a single space\n",
    "    papers_content = [re.sub(r\"\\s+\", \" \", p).lower() for p in papers_content]\n",
    "\n",
    "    # add spaces before all punctuation, so they are separate tokens\n",
    "    punctuation = set(re.findall(r\"[^\\w\\s]+\", \" \".join(papers_content))) - {\"-\",\"'\"}\n",
    "    for c in punctuation:\n",
    "        papers_content = [p.replace(c, \" \"+c+\" \") for p in papers_content]\n",
    "    papers_content = [re.sub(r\"\\s+\", \" \", p).lower().strip() for p in papers_content]\n",
    "    \n",
    "    authors = [tuple(re.findall(\"MADISON|JAY|HAMILTON\", a)) for a in papers]\n",
    "    authors = [a for a in authors if len(a) > 0]\n",
    "    \n",
    "    numbers = [re.search(r\"No\\. \\d+\", p).group(0) for p in papers if re.search(r\"No\\. \\d+\", p)]\n",
    "    \n",
    "    return papers_content, authors, numbers\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "papers, authors, numbers = load_federalist_corpus(\"pg18.txt\")\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're welcome to dig through the code here if you're curious, but it's more important that you look at the objects that the function returns.  The `papers` object is a list of strings, each one containing the full content of one of the Federalist Papers.  All tokens (words) in the text are separated by a single space (this includes some puncutation tokens, which have been modified to include spaces both before and after the punctuation. The `authors` object is a list of lists, which each list contains the author (or potential authors) of a given paper.  Finally the `numbers` list just contains the number of each Federalist paper.  You won't need to use this last one, but you may be curious to compare the results of your textual analysis to the opinion of historians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Bag of words, and TFIDF [6+6pts]\n",
    "\n",
    "In this portion of the question, you'll use a bag of words model to describe the corpus, and write routines to build a TFIDF matrix and a cosine similarity function.  Specifically, you should first implement the TFIDF function below.  This should return a _sparse_ TFIDF matrix (as for the Graph Library assignment, make sure to directly create a sparse matrix using `scipy.sparse.coo_matrix()` rather than create a dense matrix and then convert it to be sparse).\n",
    "\n",
    "Important: make sure you do _not_ include the empty token `\"\"` as one of your terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections # optional, but we found the collections.Counter object useful\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "def tfidf(docs):\n",
    "    \"\"\"\n",
    "    Create TFIDF matrix.  This function creates a TFIDF matrix from the\n",
    "    docs input.\n",
    "\n",
    "    Args:\n",
    "        docs: list of strings, where each string represents a space-separated\n",
    "              document\n",
    "    \n",
    "    Returns: tuple: (tfidf, all_words)\n",
    "        tfidf: sparse matrix (in any scipy sparse format) of size (# docs) x\n",
    "               (# total unique words), where i,j entry is TFIDF score for \n",
    "               document i and term j\n",
    "        all_words: list of strings, where the ith element indicates the word\n",
    "                   that corresponds to the ith column in the TFIDF matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    complete_docs = \" \".join(docs).split()\n",
    "    counterlist = collections.Counter(complete_docs)\n",
    "    word_list = list(counterlist)\n",
    "#     word_list.remove(\"\")\n",
    "    \n",
    "    word_dict = {k:i for i, k in enumerate(word_list)}\n",
    "    list_of_counters = [collections.Counter([word for word in doc.split()]) for doc in docs]\n",
    "    \n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "    for i in range(len(docs)):\n",
    "        for k in list_of_counters[i]:\n",
    "            if k!=\"\":\n",
    "                rows.append(i)\n",
    "                cols.append(word_dict[k])\n",
    "                data.append(list_of_counters[i][k])\n",
    "            \n",
    "    X=sp.coo_matrix((data, (rows, cols)), (len(docs), len(word_list)))\n",
    "    idf = np.log(float(len(docs))/np.asarray((X>0).sum(axis=0))[0])\n",
    "    return X*sp.diags(idf), word_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our version results the following result (just showing the type, size, and # of non-zero elements):\n",
    "\n",
    "    <86x8686 sparse matrix of type '<type 'numpy.float64'>'\n",
    "        with 57607 stored elements in Compressed Sparse Row format>\n",
    "     \n",
    "For testing, you can also run the algorithm on the following \"data set\" from class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81093022 1.09861229 0.         1.09861229 1.09861229 0.40546511\n",
      "  1.09861229 1.09861229 1.09861229 1.09861229 0.40546511 0.40546511\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.40546511 0.         0.         0.         0.         0.40546511\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.40546511 0.40546511 1.09861229 1.09861229 1.09861229 1.09861229\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.         0.         0.         0.\n",
      "  1.09861229]]\n",
      "['the', 'goal', 'of', 'this', 'lecture', 'is', 'to', 'explain', 'basics', 'free', 'text', 'processing', 'bag', 'words', 'model', 'one', 'such', 'approach', 'via']\n"
     ]
    }
   ],
   "source": [
    "### AUTOLAB_IGNORE_START\n",
    "data = [\n",
    "    \"the goal of this lecture is to explain the basics of free text processing\",\n",
    "    \"the bag of words model is one such approach\",\n",
    "    \"text processing via bag of words\"\n",
    "]\n",
    "\n",
    "X_tfidf, words = tfidf(data)\n",
    "print(X_tfidf.todense())\n",
    "print(words)\n",
    "\n",
    "### AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our implementation, this returns the following output:\n",
    "\n",
    "    [[ 0.          0.          1.09861229  1.09861229  0.          1.09861229\n",
    "       0.          0.40546511  0.40546511  1.09861229  0.          1.09861229\n",
    "       0.          0.          0.40546511  1.09861229  0.81093022  0.\n",
    "       1.09861229]\n",
    "     [ 1.09861229  1.09861229  0.          0.          0.40546511  0.          0.\n",
    "       0.40546511  0.          0.          1.09861229  0.          0.\n",
    "       0.40546511  0.          0.          0.40546511  1.09861229  0.        ]\n",
    "     [ 0.          0.          0.          0.          0.40546511  0.          0.\n",
    "       0.          0.40546511  0.          0.          0.          1.09861229\n",
    "       0.40546511  0.40546511  0.          0.          0.          0.        ]]\n",
    "    ['model', 'such', 'basics', 'goal', 'bag', 'this', 'of', 'is', 'processing', 'free', 'one', 'to', 'via', 'words', 'text', 'lecture', 'the', 'approach', 'explain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the following simple function that takes the X_tfidf matrix (though it could also take simple term frequency matrices, etc), and compute a matrix of all pair-wise cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(X):\n",
    "    \"\"\"\n",
    "    Return a matrix of cosine similarities.\n",
    "    \n",
    "    Args:\n",
    "        X: sparse matrix of TFIDF scores or term frequencies\n",
    "    \n",
    "    Returns:\n",
    "        M: dense numpy array of all pairwise cosine similarities.  That is, the \n",
    "           entry M[i,j], should correspond to the cosine similarity between the \n",
    "           ith and jth rows of X.\n",
    "    \"\"\"\n",
    "    Y = np.asarray(X@X.T.todense())\n",
    "    return Y/np.sqrt(np.diag(Y))/np.sqrt(np.diag(Y)[:,None])\n",
    "\n",
    "csm = cosine_similarity(tfidf(papers)[0])\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you apply this function to the example from class:\n",
    "\n",
    "    M = cosine_similarity(X_tfidf)\n",
    "    \n",
    "we get the result presented in the slides:\n",
    "\n",
    "    [[ 1.          0.06796739  0.07771876]\n",
    "     [ 0.06796739  1.          0.10281225]\n",
    "     [ 0.07771876  0.10281225  1.        ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use this model to analyze potential authorship of the unknown Federalist Papers.  Specifically, compute the average cosine similarity between all the _known_ Hamilton papers and all the _unknown_ papers (and similarly between known Madison and unknown, and Jay and unknown).  Populate the following variables with these averages.  As a quick check, our value for the `jay_mean_cosine_similarity` (and we know definitively that the unknown papers were _not_ written by Jay), is 0.064939."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06988141829964972\n",
      "0.0898771069881563\n",
      "0.06493942444436744\n"
     ]
    }
   ],
   "source": [
    "def count_disputes(authors):\n",
    "    disp = []\n",
    "    ham = []\n",
    "    jay = []\n",
    "    mad = []\n",
    "    for i in range(len(authors)):\n",
    "        if authors[i] == (\"HAMILTON\",):\n",
    "            ham.append(i)\n",
    "        elif authors[i] == (\"JAY\",):\n",
    "            jay.append(i)\n",
    "        elif authors[i] == (\"MADISON\",):\n",
    "            mad.append(i)\n",
    "        else:\n",
    "            disp.append(i)\n",
    "    return np.asarray(ham),np.asarray(mad),np.asarray(jay),np.asarray(disp)\n",
    "\n",
    "def convert(l):\n",
    "    return np.asarray(l)-1\n",
    "\n",
    "def mean_cos_similarity(csm, author_list, disputed_list):\n",
    "    return np.mean(csm[author_list[:, None], disputed_list])\n",
    "\n",
    "Hamilton, Madison, Jay, Disputed = count_disputes(authors)\n",
    "\n",
    "# Hamilton = [1]+list(range(6,10))+list(range(11,14))+list(range(15,18))+list(range(21,37))+list(range(59,62))+list(range(65,87))\n",
    "# Madison = [10, 14]+list(range(37,49))\n",
    "# Jay = [2,3,4,5,64]\n",
    "# Disputed = list(range(48,58))+[61,62]\n",
    "\n",
    "# Hamilton = convert(Hamilton)\n",
    "# Jay = convert(Jay)\n",
    "# Madison = convert(Madison)\n",
    "\n",
    "hamilton_mean_cosine_similarity = mean_cos_similarity(csm, Hamilton, Disputed) \n",
    "madison_mean_cosine_similarity = mean_cos_similarity(csm, Madison, Disputed)\n",
    "jay_mean_cosine_similarity = mean_cos_similarity(csm, Jay, Disputed)\n",
    "\n",
    "print(mean_cos_similarity(csm, Hamilton, Disputed))\n",
    "print(mean_cos_similarity(csm, Madison, Disputed))\n",
    "print(mean_cos_similarity(csm, Jay, Disputed))\n",
    "\n",
    "# total = np.concatenate((hamilton_mean_cosine_similarity, madison_mean_cosine_similarity, jay_mean_cosine_similarity), axis=0).reshape(3,-1)\n",
    "# np.argmax(total, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  5,  6,  7,  8, 10, 11, 12, 14, 15, 16, 20, 21, 22, 23, 24, 25,\n",
       "       26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 58, 59, 60, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "       85])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hamilton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: N-gram language models [0+10+13pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will implement an n-gram model to be able to model the language used in the Federalist Papers in a more structured manner than the simple bag of words approach.  You will fill in the following class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, docs, n):\n",
    "        \"\"\"\n",
    "        Initialize an n-gram language model.\n",
    "        \n",
    "        Args:\n",
    "            docs: list of strings, where each string represents a space-separated\n",
    "                  document\n",
    "            n: integer, degree of n-gram model\n",
    "        \"\"\"\n",
    "        self.counts = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "        self.count_sums = collections.defaultdict(int)\n",
    "        self.n = n\n",
    "        for doc in docs:\n",
    "            doc_list = doc.split()\n",
    "            for i in range(len(doc_list)-n):\n",
    "                self.count_sums[\" \".join(doc_list[i:n-1+i])] += 1\n",
    "                self.counts[\" \".join(doc_list[i:n-1+i])][doc_list[n-1+i]] +=1\n",
    "        self.dictionary = set(\" \".join(docs).split())\n",
    "        \n",
    "    def perplexity(self, text, alpha=1e-3):\n",
    "        \"\"\"\n",
    "        Evaluate perplexity of model on some text.\n",
    "        \n",
    "        Args:\n",
    "            text: string containing space-separated words, on which to compute\n",
    "            alpha: constant to use in Laplace smoothing\n",
    "            \n",
    "        Note: for the purposes of smoothing, the dictionary size (i.e, the D term)\n",
    "        should be equal to the total number of unique words used to build the model\n",
    "        _and_ in the input text to this function.\n",
    "            \n",
    "        Returns: perplexity\n",
    "            perplexity: floating point value, perplexity of the text as evaluted\n",
    "                        under the model.\n",
    "        \"\"\"\n",
    "        dictionary = self.dictionary | set(words)\n",
    "        doc_list = text.split()\n",
    "        log_prob = 0.\n",
    "        for i in range(len(doc_list)-self.n):\n",
    "            log_prob += np.log((self.counts[\" \".join(doc_list[i:i+self.n-1])][doc_list[i+self.n-1]]+alpha)/(self.count_sums[\" \".join(doc_list[i:i+self.n-1])]+len(dictionary)*alpha))\n",
    "#             print(log_prob)\n",
    "        return np.exp(-log_prob/(len(doc_list)-self.n+1))\n",
    "        \n",
    "    def sample(self, k):\n",
    "        \"\"\"\n",
    "        Generate a random sample of k words.\n",
    "        \n",
    "        Args:\n",
    "            k: integer, indicating the number of words to sample\n",
    "            \n",
    "        Returns: text\n",
    "            text: string of words generated from the model.\n",
    "        \"\"\"\n",
    "        sample_words = \"\"\n",
    "        print(sample_words)\n",
    "        for i in range(k):\n",
    "            key = random.choice(list(self.count_sums))\n",
    "            sample_words += key\n",
    "            i += (self.n-1)\n",
    "            while i<k:\n",
    "                sample_split = \" \".join(sample_words.split()[i-self.n+1:])\n",
    "                try:\n",
    "                    choice_list = random.choice(list(self.counts[sample_split]))\n",
    "                except:\n",
    "                    choice_list = None\n",
    "                if choice_list:\n",
    "                    sample_words += (\" \"+choice_list)\n",
    "                    i += 1\n",
    "                else:\n",
    "                    choice_list = random.choice(list(self.count_sums))\n",
    "                    sample_words += (\" \"+choice_list)                  \n",
    "                    i += (self.n-1)\n",
    "                if i>=k:\n",
    "                    break\n",
    "            if i>=k:\n",
    "                break\n",
    "        return sample_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.580053871133973"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_hamilton = LanguageModel(list(np.asarray(papers)[list(Hamilton)]), 3)\n",
    "# l_hamilton.counts[\"privilege of\"]\n",
    "l_hamilton.perplexity(papers[0])\n",
    "# l_hamilton.sample(200)\n",
    "# \"privilege of\".split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Initializing the language model\n",
    "\n",
    "First, implement the `__init__()` function in the `LanguageModel` class.  You can store the information any way you want, but we did this by building a two-level dictionary (in fact, we used the `collections.defaultdict` class, but this only make a few things a little bit shorter ... you are free to use it or not) `self.counts`, where the first key refers to the previous $n-1$ tokens, and the second key refers to the $n$th token, and the value simply stores the count of the number of times this combination was seen.  For ease of use in later function, we also created a `self.count_sums`, which contained the number of total times each $n-1$ combination was seen.\n",
    "\n",
    "For example, letting `l_hamilton` be a LanguageModel object built just from all the known Hamilton papers and with `n = 3`, the following varibles are populated in the object:\n",
    "\n",
    "    l_hamilton.counts[\"privilege of\"] = {'being': 1, 'originating': 1, 'paying': 1, 'the': 1}\n",
    "    l_hamilton.count_sums[\"privilege of\"] = 4\n",
    "    \n",
    "We also build a `self.dictionary` variable, which is just a `set` object containing all the unique words in across the entire set of input document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Computing perplexity\n",
    "\n",
    "Next, implement the `perplexity()` function, which takes a text sample and computes the perplexity of this sample under the model.  Use the formula for perplexity from the class nodes (which is actually not exact, since it only so, being careful to not multiply togther probabilites that get too small (hint: instead of taking the log of a large product, take the sum of the logs).\n",
    "\n",
    "You'll want to be careful about dictionary sizes when it comes to the Laplace smoothing term: make sure your dictionary size $D$ is equal to the total number of unique words that occur in either the original data used to build the language model _or_ in the text we are evaluating the perplexity of (so the size of the union of the two).\n",
    "\n",
    "As a simple example, if we build our `l_hamilton` model above (again, with `n=3`) and using default settings so that `alpha = 1e-3`, and run in on `papers[0]` (which was written by Hamilton), we get:\n",
    "\n",
    "    l_hamilton.perplexity(papers[0]) = 12.5877"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this model, evaluate the mean of the perplexity of the unknown Federalist papers for the language models from each of the three authors (again, using `n=3` and the default of `alpha=1e-3`).  Populate the following variables with the mean perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1931.6962021873007, 1764.8185010049383, 1806.264046821979)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_hamilton = LanguageModel(list(np.asarray(papers)[list(Hamilton)]), 3)\n",
    "l_madison = LanguageModel(list(np.asarray(papers)[list(Madison)]), 3)\n",
    "l_jay = LanguageModel(list(np.asarray(papers)[list(Jay)]), 3)\n",
    "\n",
    "perp_hamilton = sum([l_hamilton.perplexity(paper) for paper in list(np.asarray(papers)[list(Disputed)])])/len(list(np.asarray(papers)[list(Disputed)]))\n",
    "perp_madison = sum([l_madison.perplexity(paper) for paper in list(np.asarray(papers)[list(Disputed)])])/len(list(np.asarray(papers)[list(Disputed)]))\n",
    "perp_jay = sum([l_jay.perplexity(paper) for paper in list(np.asarray(papers)[list(Disputed)])])/len(list(np.asarray(papers)[list(Disputed)]))\n",
    "\n",
    "(perp_hamilton, perp_madison, perp_jay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1941.3851677346731 1796.913703659331 2027.895787270102\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, you should get a perplexity of 1941.38516773 for the unknown papers under the Hamilton model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Generating text\n",
    "\n",
    "Finally, implement the `sample()` function to generate random samples of text.  Essentially, you want to pick some random starting $n-1$ tuples (you can do this any way you want), then sample according to the model.  Here you should _not_ use any Laplace smoothing, but just sample from the raw underlying counts.\n",
    "\n",
    "One potential failure case, since we're just using raw counts, is if we generate an n-gram that _only_ occurs at the very end of a document (and so has no following n-gram observed in the data).  In this happens, just generate a new random set of $n-1$ tuples, and continue generating.\n",
    "\n",
    "We'll be pretty lax in grading here: we're just going to be evaluating the perplexity of the generated text and make sure it's within some very loose bounds.  This is more for the fun of seeing some generated text than for actual concrete evaluation, since it's generating a random sample.  Here's what a sample of 200 words from our Hamilton model looks like (of course all random samples will be different). \n",
    "\n",
    "    'erroneous foundation . the authorities essential to the mercy of the body politic against these two legislatures coexisted for ages in two ways : either by actual possession of which , if it cease to regard the servile pliancy of the states which have a salutary and powerful means , by those who appoint them . they are rather a source of delinquency , it would enable the national situation , and consuls , judges of their own immediate aggrandizement ? would she not have been felt by those very maxims and councils which would be mutually questions of property ? and will he not only against the united netherlands with that which has been presumed . the censure attendant on bad measures among a multitude that might have been of a regular and effective system of civil polity had previously established in this state , but upon our neutrality . by thus circumscribing the plan of opposition , and the industrious habits of the trust committed to hands which could not be likely to do anything else . when will the time at which we might soar to a deed for conveying property of the people , were dreaded and detested'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
